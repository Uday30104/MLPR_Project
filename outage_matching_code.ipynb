{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4168de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_latitude</th>\n",
       "      <th>event_longitude</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.8800</td>\n",
       "      <td>-88.7300</td>\n",
       "      <td>INDIANA</td>\n",
       "      <td>PORTER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.5000</td>\n",
       "      <td>-89.2800</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>CARROLL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31.8300</td>\n",
       "      <td>-89.0000</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>BOTETOURT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31.7100</td>\n",
       "      <td>-89.1400</td>\n",
       "      <td>VIRGINIA</td>\n",
       "      <td>CHARLOTTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.2500</td>\n",
       "      <td>-78.8000</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>CATTARAUGUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.1132</td>\n",
       "      <td>-77.6949</td>\n",
       "      <td>NEW HAMPSHIRE</td>\n",
       "      <td>STRAFFORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39.2601</td>\n",
       "      <td>-77.5801</td>\n",
       "      <td>ILLINOIS</td>\n",
       "      <td>PIATT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38.8716</td>\n",
       "      <td>-78.5255</td>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>JONES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>39.7192</td>\n",
       "      <td>-76.6533</td>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>JASPER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39.3627</td>\n",
       "      <td>-77.3865</td>\n",
       "      <td>MISSISSIPPI</td>\n",
       "      <td>JONES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   event_latitude  event_longitude          state       county\n",
       "0         39.8800         -88.7300        INDIANA       PORTER\n",
       "1         31.5000         -89.2800       VIRGINIA      CARROLL\n",
       "2         31.8300         -89.0000       VIRGINIA    BOTETOURT\n",
       "3         31.7100         -89.1400       VIRGINIA    CHARLOTTE\n",
       "4         42.2500         -78.8000       NEW YORK  CATTARAUGUS\n",
       "5         39.1132         -77.6949  NEW HAMPSHIRE    STRAFFORD\n",
       "6         39.2601         -77.5801       ILLINOIS        PIATT\n",
       "7         38.8716         -78.5255    MISSISSIPPI        JONES\n",
       "8         39.7192         -76.6533    MISSISSIPPI       JASPER\n",
       "9         39.3627         -77.3865    MISSISSIPPI        JONES"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pytz\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial import KDTree\n",
    "\n",
    "df=pd.read_csv(\"Thunderstorm_Wind.csv\")\n",
    "df_new=pd.read_csv(\"thunderstorm_var.csv\")\n",
    "\n",
    "# Prepare coordinates from both datasets\n",
    "weather_coords = df_new[['event_latitude', 'event_longitude']].to_numpy()\n",
    "tornado_coords = df[['BEGIN_LAT', 'BEGIN_LON']].dropna().to_numpy()\n",
    "\n",
    "# Build KDTree for fast nearest-neighbor lookup\n",
    "tree = KDTree(tornado_coords)\n",
    "\n",
    "# Query the nearest tornado point for each weather point\n",
    "distances, indices = tree.query(weather_coords, k=1)\n",
    "\n",
    "# Extract matched indices\n",
    "matched_tornado_data = df.iloc[indices.flatten()][['STATE', 'CZ_NAME']].reset_index(drop=True)\n",
    "\n",
    "# Add county and state to weather dataset\n",
    "df_new['state'] = matched_tornado_data['STATE']\n",
    "df_new['county'] = matched_tornado_data['CZ_NAME']\n",
    "df_new.to_csv(\"total_thunder_state.csv\", index=False)\n",
    "\n",
    "\n",
    "# Display updated dataframe\n",
    "df_new[['event_latitude', 'event_longitude', 'state', 'county']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c41c48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Year-wise files created in 'yearly_data/' folder.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('total_thunder_state.csv')\n",
    "\n",
    "# Make sure 'time' column is datetime\n",
    "df['time'] = pd.to_datetime(df['time'])\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "output_folder = 'yearly_data'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Extract year from 'time' column\n",
    "df['year'] = df['time'].dt.year\n",
    "\n",
    "# Group by year and save each year's data\n",
    "for year, group in df.groupby('year'):\n",
    "    output_path = os.path.join(output_folder, f'data_{year}.csv')\n",
    "    group.drop(columns='year').to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ Year-wise files created in 'yearly_data/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bcc12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16374) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Processing year: 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2014: 0it [00:00, ?it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 1it [00:00,  3.17it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 2it [00:00,  3.69it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 3it [00:00,  4.08it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 4it [00:00,  4.37it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 5it [00:01,  4.53it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 6it [00:01,  4.50it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 7it [00:01,  4.68it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 8it [00:01,  4.88it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 9it [00:02,  4.40it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 10it [00:02,  4.47it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2014: 11it [00:02,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2014.csv | Matches: 498\n",
      "\n",
      "🔄 Processing year: 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2015: 21it [00:04,  5.08it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2015: 40it [00:07,  5.33it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2015: 41it [00:08,  5.30it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2015: 115it [00:22,  4.98it/s]/var/folders/46/svrn09zn5wq75vy8775dkw440000gn/T/ipykernel_9628/1668799726.py:72: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
      "Processing outages 2015: 130it [00:25,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2015.csv | Matches: 8760\n",
      "\n",
      "🔄 Processing year: 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2016: 134it [00:30,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2016.csv | Matches: 9480\n",
      "\n",
      "🔄 Processing year: 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2017: 151it [00:27,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2017.csv | Matches: 7450\n",
      "\n",
      "🔄 Processing year: 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2018: 218it [00:41,  5.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2018.csv | Matches: 10428\n",
      "\n",
      "🔄 Processing year: 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2019: 241it [00:53,  4.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2019.csv | Matches: 14643\n",
      "\n",
      "🔄 Processing year: 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2020: 256it [00:58,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2020.csv | Matches: 15812\n",
      "\n",
      "🔄 Processing year: 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2021: 249it [00:45,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2021.csv | Matches: 11318\n",
      "\n",
      "🔄 Processing year: 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2022: 258it [00:51,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2022.csv | Matches: 12493\n",
      "\n",
      "🔄 Processing year: 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing outages 2023: 262it [01:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: matched_output/thunderstorm_outage_2023.csv | Matches: 15553\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Settings\n",
    "BUFFER_HOURS = 6\n",
    "START_YEAR = 2014\n",
    "END_YEAR = 2023\n",
    "OUTPUT_FOLDER = \"matched_output\"\n",
    "CHUNKSIZE = 100_000  # Adjust based on your system's memory\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Define dtypes to reduce memory usage\n",
    "outage_dtypes = {\n",
    "    'state': 'category',\n",
    "    'county': 'category',\n",
    "    'run_start_time': 'object'  # Will parse to datetime later\n",
    "}\n",
    "thunderstorm_dtypes = {\n",
    "    'state': 'category',\n",
    "    'county': 'category',\n",
    "    'event_datetime': 'object'  # Will parse to datetime later\n",
    "}\n",
    "\n",
    "for year in range(START_YEAR, END_YEAR + 1):\n",
    "    print(f\"\\n🔄 Processing year: {year}\")\n",
    "\n",
    "    # File paths\n",
    "    outage_file = f\"eaglei_data/eaglei_outages_{year}.csv\"\n",
    "    thunderstorm_file = f\"yearly_data/data_{year}.csv\"\n",
    "    output_file = os.path.join(OUTPUT_FOLDER, f\"thunderstorm_outage_{year}.csv\")\n",
    "\n",
    "    # Skip if either file is missing\n",
    "    if not (os.path.exists(outage_file) and os.path.exists(thunderstorm_file)):\n",
    "        print(f\"⚠️ Missing files for {year}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Load thunderstorm data (assumed smaller, load entirely)\n",
    "    try:\n",
    "        thunderstorm_df = pd.read_csv(thunderstorm_file, dtype=thunderstorm_dtypes)\n",
    "        thunderstorm_df['event_datetime'] = pd.to_datetime(\n",
    "            thunderstorm_df['event_datetime'], format='%d/%m/%y %H:%M', errors='coerce'\n",
    "        )\n",
    "        if thunderstorm_df['event_datetime'].isna().sum() > 0:\n",
    "            print(f\"⚠️ {thunderstorm_df['event_datetime'].isna().sum()} invalid dates in thunderstorm data for {year}\")\n",
    "        thunderstorm_df['county'] = thunderstorm_df['county'].str.strip().str.lower()\n",
    "        thunderstorm_df['state'] = thunderstorm_df['state'].str.strip().str.lower()\n",
    "        thunderstorm_df['key'] = thunderstorm_df['state'] + \"_\" + thunderstorm_df['county']\n",
    "        thunderstorm_df['event_id'] = thunderstorm_df.index  # Unique ID for each thunderstorm event\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in thunderstorm data for {year}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Initialize results list for matched data\n",
    "    matched_results = []\n",
    "\n",
    "    # Process outage data in chunks\n",
    "    try:\n",
    "        # Only load required columns\n",
    "        outage_chunks = pd.read_csv(\n",
    "            outage_file,\n",
    "            dtype=outage_dtypes,\n",
    "            usecols=['state', 'county', 'run_start_time'],\n",
    "            chunksize=CHUNKSIZE\n",
    "        )\n",
    "        for chunk in tqdm(outage_chunks, desc=f\"Processing outages {year}\"):\n",
    "            # Parse datetime\n",
    "            chunk['run_start_time'] = pd.to_datetime(chunk['run_start_time'], errors='coerce')\n",
    "            if chunk['run_start_time'].isna().sum() > 0:\n",
    "                print(f\"⚠️ {chunk['run_start_time'].isna().sum()} invalid dates in outage chunk for {year}\")\n",
    "\n",
    "            # Normalize location\n",
    "            chunk['county'] = chunk['county'].str.strip().str.lower()\n",
    "            chunk['state'] = chunk['state'].str.strip().str.lower()\n",
    "            chunk['key'] = chunk['state'] + \"_\" + chunk['county']\n",
    "\n",
    "            # Merge with thunderstorm data\n",
    "            merged_df = pd.merge(\n",
    "                thunderstorm_df,\n",
    "                chunk,\n",
    "                on='key',\n",
    "                suffixes=('_thunderstorm', '_outage')\n",
    "            )\n",
    "\n",
    "            # Filter by time difference\n",
    "            time_diff = merged_df['run_start_time'] - merged_df['event_datetime']\n",
    "            valid_time = (time_diff >= timedelta(0)) & (time_diff <= timedelta(hours=BUFFER_HOURS))\n",
    "            filtered_df = merged_df[valid_time]\n",
    "\n",
    "            if not filtered_df.empty:\n",
    "                # Aggregate per thunderstorm event, only tracking event_id\n",
    "                grouped = filtered_df[['event_id']].drop_duplicates().reset_index(drop=True)\n",
    "                grouped['caused_power_outage'] = 1\n",
    "                matched_results.append(grouped)\n",
    "\n",
    "            # Free memory\n",
    "            del chunk, merged_df, filtered_df\n",
    "            gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in outage data for {year}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Combine matched results\n",
    "    if matched_results:\n",
    "        final_matched = pd.concat(matched_results).groupby('event_id').agg({\n",
    "            'caused_power_outage': 'max'\n",
    "        }).reset_index()\n",
    "    else:\n",
    "        final_matched = pd.DataFrame(columns=['event_id', 'caused_power_outage'])\n",
    "\n",
    "    # Merge back into thunderstorm_df\n",
    "    thunderstorm_df = pd.merge(thunderstorm_df, final_matched, on='event_id', how='left')\n",
    "    thunderstorm_df['caused_power_outage'] = thunderstorm_df['caused_power_outage'].fillna(0).astype(int)\n",
    "\n",
    "    # Drop temp columns and save\n",
    "    thunderstorm_df.drop(columns=['event_id', 'key'], inplace=True)\n",
    "    thunderstorm_df.to_csv(output_file, index=False)\n",
    "    print(f\"✅ Saved: {output_file} | Matches: {thunderstorm_df['caused_power_outage'].sum()}\")\n",
    "\n",
    "    # Cleanup\n",
    "    del thunderstorm_df, final_matched, matched_results\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2525541a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined 10 files into one DataFrame with 162229 rows.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Path to the folder containing the CSV files\n",
    "folder_path = 'matched_output'\n",
    "\n",
    "# Create a list of file paths for the years 2014 to 2023\n",
    "csv_files = [os.path.join(folder_path, f'thunderstorm_outage_{year}.csv') for year in range(2014, 2024)]\n",
    "\n",
    "# Read and concatenate all the CSV files\n",
    "combined_df = pd.concat([pd.read_csv(file) for file in csv_files if os.path.exists(file)], ignore_index=True)\n",
    "\n",
    "# Optional: Save the combined DataFrame to a new CSV\n",
    "combined_df.to_csv('thunderstorm_outage_combined_2014_2023.csv', index=False)\n",
    "\n",
    "print(f\"Combined {len(csv_files)} files into one DataFrame with {len(combined_df)} rows.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce651766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
